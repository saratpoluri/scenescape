# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
# SPDX-License-Identifier: LicenseRef-Intel-Edge-Software
# This file is licensed under the Limited Edge Software Distribution License Agreement.

import sys
from abc import ABCMeta, abstractmethod
from copy import copy
import inspect
from enum import Flag, auto
import numpy as np
import torch
from types import SimpleNamespace as SN
from torch import nn

torch.set_flush_denormal(True)
torch.jit.enable_onednn_fusion(True)


class BaseModel(nn.Module, metaclass=ABCMeta):
  default_conf = {}
  required_inputs = []

  def __init__(self, conf):
    """Perform some logic and call the _init method of the child model."""
    super().__init__()
    self.conf = conf = {**self.default_conf, **conf}
    self.required_inputs = copy(self.required_inputs)
    self._init(conf)
    sys.stdout.flush()

  def forward(self, data):
    """Check the data and call the _forward method of the child model."""
    for key in self.required_inputs:
      assert key in data, "Missing key {} in data".format(key)
    return self._forward(data)

  @abstractmethod
  def _init(self, conf):
    """To be implemented by the child class."""
    raise NotImplementedError

  @abstractmethod
  def _forward(self, data):
    """To be implemented by the child class."""
    raise NotImplementedError


def dynamic_load(root, model):
  module_path = f"{root.__name__}.{model}"
  module = __import__(module_path, fromlist=[""])
  classes = inspect.getmembers(module, inspect.isclass)
  # Filter classes defined in the module
  classes = [c for c in classes if c[1].__module__ == module_path]
  # Filter classes inherited from BaseModel
  classes = [c for c in classes if issubclass(c[1], BaseModel)]
  assert len(classes) == 1, classes
  return classes[0][1]
  # return getattr(module, 'Model')


def cached_load(root, model_conf):
  """Reuse optimized model in repeat calls. Only maintains one model per name, so
  using a different configuration will overwrite a model."""
  name = model_conf["name"]
  if (
      name in cached_load.model_cache
      and cached_load.model_cache[name].conf == model_conf
  ):
    return cached_load.model_cache[name].model
  Model = dynamic_load(root, model_conf["name"])
  model = Model(model_conf).eval().to(cached_load.device)
  if "optimize" in model_conf and "script" in model_conf["optimize"]:
    model = torch.jit.freeze(torch.jit.script(model))
    model = torch.jit.optimize_for_inference(model)
  if cached_load.device == "cpu":
    try:
      import intel_extension_for_pytorch as ipex

      ipex.enable_onednn_fusion(True)

      model = ipex.optimize(model)
    except ImportError:
      pass
  cached_load.model_cache[name] = SN(model=model, conf=model_conf)
  return model


cached_load.device = "cuda" if torch.cuda.is_available() else "cpu"
cached_load.model_cache = {}
"""Cache loaded, optimized models by name"""
