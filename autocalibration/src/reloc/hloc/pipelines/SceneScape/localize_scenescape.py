# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
# SPDX-License-Identifier: LicenseRef-Intel-Edge-Software
# This file is licensed under the Limited Edge Software Distribution License Agreement.

from pathlib import Path
from types import SimpleNamespace
from typing import Dict, Sequence, Union
import pickle
import numpy as np
from scipy.spatial.transform import Rotation as R
import h5py
import pycolmap
import open3d as o3d
from hloc import logger
from hloc.utils.evaluate import evaluate, find_nearest_pose
from hloc.utils.parsers import parse_retrieval, names_to_pair
from hloc.utils.read_write_model import Camera, qvec2rotmat
from hloc.utils.geometry import (
    get_intrinsic_mat,
    interpolate_mesh,
    interpolate_scan,
    pose_matrix_from_qvec_tvec,
)

Tensor = o3d.core.Tensor
Image = o3d.t.geometry.Image
RGBDImage = o3d.t.geometry.RGBDImage
PointCloud = o3d.t.geometry.PointCloud


def qxyzw_to_qwxyz(qxyzw: np.ndarray):
  qwxyz = np.empty_like(qxyzw)
  qwxyz[..., 1:] = qxyzw[..., :3]
  qwxyz[..., 0] = qxyzw[..., 3]
  return qwxyz


def qwxyz_to_qxyzw(qwxyz: np.ndarray):
  qxyzw = np.empty_like(qwxyz)
  qxyzw[..., :3] = qwxyz[..., 1:]
  qxyzw[..., 3] = qwxyz[..., 0]
  return qxyzw


def qxyzwtinv(qxyzw: np.ndarray, tvec: np.ndarray):
  qinv = qxyzw.copy()
  qinv[:3] = -qinv[:3]
  tinv = -R.from_quat(qinv).apply(tvec)
  return qinv, tinv


def pose_from_cluster(
    dataset_dir: Path,
    db_feature_files: Sequence[h5py.File],
    q: str,
    retrieved: Sequence[str],
    feature_files: Sequence[h5py.File],
    match_files: Sequence[h5py.File],
    retrieval_calibration: Dict[str, SimpleNamespace],
    query_intrinsics: Dict,
    skip=0,
    match_dense: Union[bool, Sequence[bool]] = False,
    depth_scale=1.0,
    depth_max=9.9,
):
  """Estimate camera pose from a cluster of retrieved calibrated images and
  matching keypoints.

  Args:
      dataset_dir: Path to dataset.
      db_feature_files: List of files with 2D keypoints in database.
      q: Query image filename.
      retrieved: List of retrieved database images files.
      feature_files: Sequence[h5py.File].
      match_files: Sequence[h5py.File].
      retrieval_calibration: Dict[str, SimpleNamespace].
      query_intrinsics: Dict.
      skip: Skip matching an image if n_keypoints < skip.
      match_dense: Semi dense matching with a single model for both feature
          detection and matching.
      depth_scale: Depth scale, if depth is read from png images.
      depth_max: Max cutoff depth, if depth is read from png images.
  """

  all_mkpq = []
  all_mkpr = []
  all_mkp3d = []
  all_indices = []
  if isinstance(match_dense, bool):
    match_dense = (match_dense,) * len(feature_files)
  kpqs = tuple(
      ff[q]["keypoints"].__array__().astype(np.float32) for ff in feature_files
  )  # read all keypoint types
  num_matches = 0

  for i, r in enumerate(retrieved):
    pair = names_to_pair(q, r)
    mkpq, mkpr = [], []
    for md, ff, mf, kpq in zip(match_dense, db_feature_files, match_files, kpqs):
      if md:
        mkpq.append(mf[pair]["keypoints0"].__array__().astype(np.float32))
        mkpr.append(mf[pair]["keypoints1"].__array__().astype(np.float32))
      else:
        kpr = ff[r]["keypoints"].__array__().astype(np.float32)
        m = mf[pair]["matches0"].__array__()
        v = m > -1
        # Check if valid matches exist to index into keypoints array
        if len(kpq) != len(v):
          continue
        mkpq.append(kpq[v])
        mkpr.append(kpr[m[v]])
    # Avoid stacking of an empty array
    if len(mkpq) == 0:
      continue
    mkpq = np.vstack(mkpq)  # TODO: non-maxima suppression
    mkpr = np.vstack(mkpr)
    if mkpr.shape[0] < skip:
      continue

    num_matches += len(mkpq)

    depth_type = retrieval_calibration[r].depth_name.split(".")[1]
    if depth_type in ("ply", "stl", "obj", "fbx", "gltf", "glb"):
      Tcw = pose_matrix_from_qvec_tvec(
          retrieval_calibration[r].qvec, retrieval_calibration[r].tvec
      )
      mkp3d, valid = interpolate_mesh(
          Path(dataset_dir, retrieval_calibration[r].depth_name),
          Tcw,
          retrieval_calibration[r].intrinsics,
          mkpr,
      )

    elif depth_type in ("hdf5", "h5"):
      depth_file = h5py.File(
          Path(dataset_dir, retrieval_calibration[r].depth_name), "r"
      )
      depth_r = Image(
          Tensor.from_numpy(next(iter(depth_file.values()))[...])
      )  # first dataset as depth
    elif depth_type == "png":
      depth_r = o3d.t.io.read_image(
          str(Path(dataset_dir, retrieval_calibration[r].depth_name))
      )
      depth_r = depth_r.clip_transform(
          scale=depth_scale, min_value=0.1, max_value=depth_max, clip_fill=np.nan
      )

    if depth_type in ("hdf5", "h5", "png"):
      mkp3d, valid = interpolate_scan(
          depth_r, retrieval_calibration[r].intrinsics, mkpr
      )
      # Rw_c, tw_c:  camera -> world
      Rw_c = qvec2rotmat(retrieval_calibration[r].qvec).T
      tw_c = -Rw_c @ np.array(retrieval_calibration[r].tvec).reshape((3, 1))
      mkp3d = (Rw_c @ mkp3d.T + tw_c).T

    all_mkpq.append(mkpq[valid])
    all_mkpr.append(mkpr[valid])
    all_mkp3d.append(mkp3d[valid])
    all_indices.append(np.full(np.count_nonzero(valid), i))

  if num_matches <= 4:
    return (
        {"success": False, "cfg": query_intrinsics},
        [],
        [],
        [],
        [],
        num_matches,
    )
  all_mkpq = np.concatenate(all_mkpq, 0)
  all_mkpr = np.concatenate(all_mkpr, 0)
  all_mkp3d = np.concatenate(all_mkp3d, 0)
  all_indices = np.concatenate(all_indices, 0)

  ret = pycolmap.absolute_pose_estimation(
      all_mkpq, all_mkp3d, query_intrinsics, max_error_px=48.00
  )
  ret["cfg"] = query_intrinsics
  return ret, all_mkpq, all_mkpr, all_mkp3d, all_indices, num_matches


def read_cameras_text(path):
  """Read camera intrinsics. See format below.

  Also see colmap: src/base/reconstruction.cc
      void Reconstruction::WriteCamerasText(const std::string& path)
      void Reconstruction::ReadCamerasText(const std::string& path)

  `#camera_id model width height params`

  model is one of:

  - SIMPLE_PINHOLE: params are f, cx, cy
  - PINHOLE: params are fx, cx, cy
  - OPENCV: params are fx, fy, cx, cy, k1, k2, p1, p2

  Args:
      path: Path to cameras file.

  Returns:
      Dict mapping camera_id to Camera objects
  """
  cameras = {}
  with open(path, "r") as fid:
    line = fid.readline().strip()
    if "#camera_id model width height params" not in line:
      raise ValueError(
          "Invalid cameras file, expected format:\n"
          "#camera_id model width height params\n"
          "got format:\n" + line
      )
    line = fid.readline().strip()
    while line:
      if len(line) > 0 and line[0] != "#":
        elems = line.split()
        camera_id = elems[0]  # str instead of int
        model = elems[1].upper()
        width = int(elems[2])
        height = int(elems[3])
        params = np.array(tuple(map(float, elems[4:])))
        cameras[camera_id] = Camera(
            id=camera_id, model=model, width=width, height=height, params=params
        )
      line = fid.readline().strip()
  return cameras


def read_images_extrinsics(extrinsics_path: Path):
  """Read image extrinsics. Text file format:

  `#image_name camera_id qx qy qz qw tx ty tz`

  **Note**: Rotation and translation specify the camera extrinsic transform
  (world_to_camera)

  Args:
      extrinsics_path: Path to images file.

  Returns:
      Dict mapping image_name to namespace with attributes image_name, qvec
      (quaternion rotation), tvec (translation vector) and camera_id.
  """
  images = {}
  with open(extrinsics_path, "r") as fid:
    line = fid.readline().strip()
    if "#image_name camera_id qx qy qz qw tx ty tz" not in line:
      raise ValueError(
          "Invalid images file, expected format:\n"
          "#image_name camera_id qx qy qz qw tx ty tz\n"
          "got format:\n" + line
      )
    line = fid.readline().strip()
    while line:
      if len(line) > 0 and line[0] != "#":
        elems = line.split()
        image_name = elems[0]
        camera_id = elems[1]
        qvec = np.array(tuple(map(float, elems[2:6])))
        qvec = qxyzw_to_qwxyz(qvec)
        tvec = np.array(tuple(map(float, elems[6:9])))
        images[image_name] = SimpleNamespace(
            name=image_name, qvec=qvec, tvec=tvec, camera_id=camera_id
        )
      line = fid.readline().strip()
  return images


def read_calibration(
    dataset_dir: Path,
    rgb_ext: str = "jpg",
    depth_ext: str = ".png",
    mesh_file: str = None,
):
  """Read camera calibration information for all dataset images.

  Args:
      dataset_dir: Path containing cameras.txt (camera intrinsics) and
          images.txt (image extrinsics) text files.
      rgb_ext: RGB images file extension (e.g. 'jpg')
      depth_ext: Depth images or 3D mesh file extension (e.g. '.png' or '.ply')
      mesh_file: Use this mesh file instead of depth images.

  Returns:
      Dict mapping image_name to namespace with attributes image_name,
      depth_name, qvec (quaternion rotation), tvec (translation vector),
      camera_id and Camera (intrinsics)
  """
  cameras = read_cameras_text(dataset_dir / "cameras.txt")
  calibration = read_images_extrinsics(dataset_dir / "images.txt")
  for cal in calibration.values():
    cal.intrinsics = cameras[cal.camera_id]
    rgbn = cal.name.split("/")
    if mesh_file is None:
      cal.depth_name = "/".join(
          (
              *rgbn[:-2],
              rgbn[-2].replace("rgb", "depth"),
              rgbn[-1].replace(rgb_ext, depth_ext),
          )
      )
    else:
      cal.depth_name = mesh_file
  return calibration


def write_calibration_text(
    out_dir: Path,
    cameras: Dict = None,
    images: Dict = None,
    single_camera=False,
    model_scale: float = 1.0,
):
  """Save calibration data as "cameras.txt" and "images.txt"
  cameras.txt file format:
      `#camera_id model width height params`

  model is one of:

  - SIMPLE_PINHOLE: params are f, cx, cy
  - PINHOLE: params are fx, cx, cy
  - OPENCV: params are fx, fy, cx, cy, k1, k2, p1, p2

  images.txt file format:
      `#image_name camera_id qx qy qz qw tx ty tz`

  Args:
      out_dir: Output directory
      cameras: camera intrinsic parameters. Skip writing if None.
      images: camera extrinsic parametsers. Skip writing if None.
      single_camera: Only use the first camera
      model_scale: Optional value to scale up the model (translation)
  """
  if cameras is not None:
    with open(out_dir / "cameras.txt", "w", encoding="utf-8") as camfile:
      camfile.write("#camera_id model width height params\n")
      for camera in cameras.values():
        params = " ".join(map(str, camera.params))
        camfile.write(
            f"{camera.id} {camera.model} {camera.width} "
            f"{camera.height} {params}\n"
        )
        if single_camera:
          camera_id = camera.id
          break

  if images is not None:
    with open(out_dir / "images.txt", "w", encoding="utf-8") as imfile:
      imfile.write("#image_name camera_id qx qy qz qw tx ty tz\n")
      for image in images.values():
        qvec = " ".join(map(str, qwxyz_to_qxyzw(image.qvec)))
        tvec = " ".join(map(str, model_scale * np.array(image.tvec)))
        camid = camera_id if single_camera else image.camera_id
        imfile.write(" ".join((image.name, str(camid), qvec, tvec)) + "\n")


def main(
    dataset_dir,
    db_feature_paths,
    retrieval,
    query_intrinsics: Dict,
    features: Sequence[Path],
    matches: Sequence[Path],
    results_path,
    skip_matches,
    match_dense,
    data_config,
):

  assert retrieval.exists(), retrieval
  assert all(f.exists() for f in features), features
  assert all(m.exists() for m in matches), matches
  result = SimpleNamespace()

  retrieval_dict = parse_retrieval(retrieval)
  query = next(iter(retrieval_dict.keys()))
  retrieval_calibration = read_calibration(
      dataset_dir, data_config.rgb_ext, data_config.depth_ext, data_config.mesh_file
  )
  have_gt = False

  if (
      "gt_query_cameras" in data_config
      and data_config.gt_query_cameras is not None
      and "gt_query_images" in data_config
      and data_config.gt_query_images is not None
  ):
    query_cameras = read_cameras_text(data_config.gt_query_cameras)
    query_poses = read_images_extrinsics(data_config.gt_query_images)
    have_gt = True
    min_retrieval_dset, min_retrieval_dist = find_nearest_pose(
        query_poses, retrieval_calibration
    )

  feature_files = tuple(h5py.File(f, "r") for f in features)
  match_files = tuple(h5py.File(m, "r") for m in matches)

  def open_or_skip_h5(filepath, mode="r"):
    try:
      return h5py.File(filepath, mode)
    except OSError:
      return None

  db_feature_files = tuple(open_or_skip_h5(f, "r") for f in db_feature_paths)

  logs = {"features": features, "matches": matches, "retrieval": retrieval, "loc": {}}
  logger.info("Starting localization...")
  retrieved = retrieval_dict[query]
  ret, mkpq, mkpr, mkp3d, indices, num_matches = pose_from_cluster(
      dataset_dir,
      db_feature_files,
      query,
      retrieved,
      feature_files,
      match_files,
      retrieval_calibration,
      query_intrinsics,
      skip_matches,
      match_dense,
      data_config.depth_scale,
      data_config.depth_max,
  )

  result.success = ret["success"]
  result.num_matches = num_matches
  result.n_keypoints = len(mkpq)
  if ret["success"]:
    result.qvec = ret["qvec"]
    result.tvec = ret["tvec"]
    result.n_inliers = ret["num_inliers"]
  else:
    result.qvec = retrieval_calibration[retrieved[0]].qvec
    result.tvec = retrieval_calibration[retrieved[0]].tvec
    result.n_inliers = -1

  logs["loc"] = {
      query: {
          "db": retrieved,
          "PnP_ret": ret,
          "keypoints_query": mkpq,
          "keypoints_db": mkpr,
          "3d_points": mkp3d,
          "indices_db": indices,
          "num_matches": num_matches,
      }
  }
  if have_gt:
    logger.info("Evaluating...")
    evaluate(
        {query: (result.qvec, result.tvec)},
        {query: (query_poses[query].qvec, query_poses[query].tvec)},
        error_basis={query: min_retrieval_dist[query]},
        logs=logs,
    )

  with open(results_path, "a") as fres:
    fres.write(
        "#query_image qw qx qy qz tx ty tz e_t(cm) e_t_rel(%) e_R(deg)"
        " success n_keypoints n_matches n_inliers\n"
    )
    qvec = " ".join(map(str, result.qvec))
    tvec = " ".join(map(str, result.tvec))
    if have_gt:
      e_t = logs["loc"][query]["e_t"]
      e_t_rel = logs["loc"][query]["e_t_rel"]
      e_R = logs["loc"][query]["e_R"]
      eval_str = f" {e_t*100:.2f} {e_t_rel*100:.2f} {e_R:.2f} "
    else:
      eval_str = " -1 -1 -1 "
    fres.write(
        f"{query} {qvec} {tvec} {eval_str} {result.success} {result.n_keypoints} "
        f"{result.num_matches} {result.n_inliers}\n"
    )
    print(
        f"{query} {eval_str} {result.success} {result.n_keypoints} "
        f"{result.num_matches} {result.n_inliers}\n"
    )
  logs_path = f"{results_path}_logs.pkl"
  with open(logs_path, "wb") as flog:
    pickle.dump(logs, flog)

  if "gt_mesh_file" in data_config and data_config.gt_mesh_file is not None:
    intrinsic_matrix = get_intrinsic_mat(query_intrinsics).numpy()
    extrinsic_matrix = pose_matrix_from_qvec_tvec(result.qvec, result.tvec)
    render = o3d.visualization.rendering.OffscreenRenderer(
        query_intrinsics.width, query_intrinsics.height
    )
    if data_config.gt_mesh_file[:-4] in (".ply", ".pcd"):
      model = o3d.io.read_point_cloud(str(data_config.gt_mesh_file))
      mat = o3d.visualization.rendering.MaterialRecord()
      render.scene.add_geometry("model", model, mat)
    else:  # '.glb', '.gltf', '.obj'
      model = o3d.io.read_triangle_model(str(data_config.gt_mesh_file))
      render.scene.add_model("model", model)
    render.setup_camera(
        intrinsic_matrix,
        extrinsic_matrix,
        query_intrinsics.width,
        query_intrinsics.height,
    )
    im = render.render_to_image()
    o3d.io.write_image(str(results_path.parent / f"render-{query}"), im)

  result.qvec = qwxyz_to_qxyzw(result.qvec)
  # camera extrinsic (world_to_camera) -> camera location (camera_to_world)
  result.qvec, result.tvec = qxyzwtinv(result.qvec, result.tvec)
  return result
